# LLM Fine-Tuning Project

This project contains code and configuration files for fine-tuning a Large Language Model (LLM) using PEFT (Parameter-Efficient Fine-Tuning) techniques.

## ğŸ“ Project Structure

```
llm-finetuning/
â”œâ”€â”€ data/                    # Training and evaluation datasets
â”œâ”€â”€ model/                   # Base model and fineâ€‘tuned model storage
â”œâ”€â”€ scripts/                 # Training, evaluation, and utility scripts
â”œâ”€â”€ configs/                 # Adapter and training configuration files
â”œâ”€â”€ adapter_config.json      # PEFT adapter configuration
â”œâ”€â”€ adapter_model.bin        # Trained adapter weights
â”œâ”€â”€ train.py                 # Script for fineâ€‘tuning
â”œâ”€â”€ evaluate_model.py        # Script for evaluation
â””â”€â”€ README.md                # Documentation
```

## ğŸš€ Features

* Fineâ€‘tuning using PEFT (LoRA/Adapter-based approach)
* Evaluation script for checking model performance
* Config files for easy reproducibility
* Supports HuggingFace models and datasets

## ğŸ”§ Requirements

Install all dependencies:

```
pip install -r requirements.txt
```

## ğŸ‹ï¸â€â™‚ï¸ Training

Run the training script:

```
python train.py
```

You can modify parameters in `configs/` as needed.

## ğŸ“ˆ Evaluation

```
python evaluate_model.py
```

This loads the fineâ€‘tuned adapter (`adapter_model.bin`) and evaluates the model.

## ğŸ§© Common Issue

**Error: adapter_model.bin paste nahi ho raha / load nahi ho raha?**

* Ensure the path in your script matches the location of `adapter_model.bin`
* The directory must contain BOTH `adapter_config.json` and `adapter_model.bin`
* Example:

```
model = PeftModel.from_pretrained(base_model, "./model/adapters/")
```

## ğŸ“œ License

This project is for educational and experimental use.

## ğŸ™Œ Author

Shubham â€“ Fineâ€‘tuning LLM experiments.
